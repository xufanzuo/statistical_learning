# Chapter1 统计学习方法概论
## 1.1 统计学习
* 统计学习(statistical learning)：是关于计算机基于数据结构构建概率统计模型并运用模型对数据进行预测与分析的一门学科。
* statistical learning 的学习对象是数据(data)。从数据出发，提取数据的特征，抽象出数据的模型，然后回到对数据的分析与预测。
* statistical learning由以下几个构成：
  * supervised learning 监督学习
  * unsupervised learning 非监督学习
  * semisupervised learning 半监督学习
  * reinforcement learning  强化学习
* supervised learning : 从 training data 集合出发，假设数据是独立同分布产生的；并且要学习的模型是某个函数的集合，称为假设空间(hypothesis space);应用某个评价准则(evaluation criterion),从假设空间选取一个最优的模型，应用于test data.

## 1.2 Supervised Learning
1. 输入空间、特征空间与输出空间
在监督学习中，将输入与输出所有可能的取值的集合分别称为 input space 和 output space。
每个具体的输入是一个 实例(instance)，通常由 特征向量(feature vector),所有的 feature vecture 存在的空间称为 feature space . 特征空间的每一维对应于一个特征。输入实例x的特征向量记作：
$$ x = (x^{(1)},x^{(2)},...,x^{(i)},...,x^{(n)})^T $$
训练集：
$$ T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\} $$

2. 联合概率分布
    监督学习假设输入与输出的随机变量 X 和 Y 遵循联合概率分布 $P(X,Y)$。$P(X,Y)$表示分布函数，或分布密度函数。

3. 假设空间
   监督学习的目的在于学习一个由输入到输出的映射，这个映射的集合就是假设空间(hypothesis space).
   监督学习的模型可以是概率模型或非概率模型，由条件概率分布$P(X,Y)$或决策函数(decision function) $Y=f(x)$表示。

## 1.3 统计学习三要素
统计学习方法都是由模型、策略和算法构成。
方法 = 模型 + 策略 + 算法

### 1.3.1模型
在 supervised learning 中，模型就是所要学习的条件概率分布或决策函数。模型的hypothesis space包含所有可能的条件概率分布或决策函数。

### 1.3.2 策略
1. 损失函数和风险函数
 supervised learning的问题是在hypothesis space中选取模型f作为决策函数，对于给定的输入X,由$f(x)$给出相应的输出Y,这个输出的预测值f(x)和真实值Y可能不一致，用一个损失函数(loss function)或代价函数(cost function)来度量预测错误的程度。损失函数是f(x)和Y的非负实值函数，记住L(Y,f(x))
 常用损失函数：
(1) 0-1 loss function
$$ L(Y,f(X)) = \begin{cases}
    1 & Y\neq f(X) \\
    0 & Y=f(X)
\end{cases} $$
(2) quadratic loss function
$$ L(Y,f(X))=(Y-f(X))^2 $$
(3) absolute loss function
$$ L(Y,f(X))=|Y - f(X) |$$
(4)loglikelihood loss function 对数似然损失函数
$$ L(Y,P(Y|X)) = -logP(Y|X) $$
损失函数值越小，模型就越好，由于模型的输入、输出(X,Y)是随机变量，遵循联合分布P(X,Y),所以损失函数的期望是：
(期望比如：抛硬币正反为0，1，则 $P\{X=x_k\}=p_k,k=1,2. 期望E(X)= \sum_{k=1}^\infty x_kp_k=0.5$)

$$ R_{exp}(f)=E_P[L(Y,f(x))]=\int_{x*y} L(y,f(x))P(x,y)dxdy$$
这个是理论上模型f(x)关于联合分布P(X,Y)的平均意义下的损失，称为风险函数(risk function)
学习的目标就是选择期望风险最小的模型.由于联合分布P(X,Y)是未知的，$R_{exp}(f)$不能直接计算，实际上，如果知道联合分布 $P（X，Y）$就可以求出 $P（Y|X）$，不需要学习。正因为不知道联合概率分布，所以才需要学习。这样一来，一方面根据期望风险最小学习模型要用到联合分布，另外一方面联合分布位置，所以监督学习是一个 病态问题（ill-formed problem)
模型 $f(x)$ 关于训练数据集的平均损失为 经验风险（empirical risk) 或 经验损失 （empirical loss），记作 $R_{emp}$:
$$ R_{emp}(f)=1/N\sum_{i=1}^NL(y_i,f(x_i)) $$

期望风险 $R_{exp}(f)$ 是模型关于联合分布的期望损失，经验风险 $R_{emp}(f)$ 是模型关于训练样本集的平均损失， 根据 **大数定律** ，当样本容量N趋于无穷时，经验风险 $R_{emp}(f)$ （实际抛硬币的结果）趋于期望风险 $R_{exp}(f)$ （概率算出的）。所以用经验风险估计期望风险，但是样本有限，需要对经验风险进行一定的校正。这就关系到监督学习的两个基本策略：**经验风险最小化** 和 **结构风险最小化**。

2. **经验风险最小化** 和 **结构风险最小化**
在假设空间、 损失函数以及训练数据集确定的情况下， 经验风险函数式就可以确定。 经验风险最小化（ empirical risk minimization， ERM） 的策略认为， 经验风险最小的模型是最优的模型。根据这一策略， 按照 **经验风险最小化** 求最优模型就是求解最优化问题：
$$ min 1/N\sum_{i=1}^NL(y_i,f(x_i))$$
当样本容量足够大时，**经验风险最小化** 能保证有很好的学习效果，在现实中被广泛采用。当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生后面将要叙述的“过拟合(over-fitting)”现象（过拟合会产生很多高阶项）.
**结构风险最小化**（ structural risk minimization， SRM）是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化（regularization).结构风险是在经验风险上加上表示模型复杂度的正则化（regularier) 或 罚项。**结构风险最小化**：
$$ R_{srm}(f)=1/N\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f) $$
其中 J(f)为模型复杂度，$\lambda$是系数，用来权衡经验风险和模型复杂度。结构风险小需要经验风险与模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。

### 1.3.3 算法
算法是指学习模型的具体计算方法，统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后考虑用什么样的计算方法求解最优模型。
这时， 统计学习问题归结为最优化问题， 统计学习的算法成为求解最优化问题的算法。